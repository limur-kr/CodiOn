import sys
import os
import torch

# ---------------------------------------------------------
# ðŸ› ï¸ [ê²½ë¡œ ì—°ê²°] íŒ€ì›ì˜ 'ml' í´ë” ìœ„ì¹˜ ì°¾ê¸° (ë¡œì»¬ vs Docker)
# ---------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))

# 1. ë¡œì»¬ í™˜ê²½ ê²½ë¡œ (í˜„ìž¬ ìœ„ì¹˜ì—ì„œ 3ì¹¸ ìœ„ -> ratio_based)
# ìœ„ì¹˜: CodiON/ai/ratio_based
local_target_path = os.path.abspath(os.path.join(current_dir, "../../../ratio_based"))

# 2. Docker í™˜ê²½ ê²½ë¡œ (Dockerfile ì„¤ì • ê¸°ì¤€)
# ìœ„ì¹˜: /app/models/ratio_based
docker_target_path = "/app/models/ratio_based"

# 3. ì¡´ìž¬í•˜ëŠ” ê²½ë¡œë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
if os.path.exists(docker_target_path):
    if docker_target_path not in sys.path:
        sys.path.append(docker_target_path)
        print(f"ðŸ³ Docker í™˜ê²½: '{docker_target_path}' ê²½ë¡œ ì¶”ê°€ë¨")
elif os.path.exists(local_target_path):
    if local_target_path not in sys.path:
        sys.path.append(local_target_path)
        print(f"ðŸ“‚ ë¡œì»¬ í™˜ê²½: '{local_target_path}' ê²½ë¡œ ì¶”ê°€ë¨")
else:
    print(f"âš ï¸ ê²½ê³ : íŒ€ì› ëª¨ë¸ í´ë”(ratio_based)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
# ---------------------------------------------------------

# ë¹¨ê°„ ì¤„ì´ ë– ë„ ì‹¤ì œ ì‹¤í–‰ì—ëŠ” ë¬¸ì œ ì—†ìŠµë‹ˆë‹¤. (IDE ì¸ì‹ ë¶ˆê°€ ë¬¸ì œ)
try:
    from ml.core.models.comfort_mlp import ComfortMLP  # type: ignore
except ImportError as e:
    print(f"ðŸ”¥ Import Error: {e}")
    ComfortMLP = None

# config ë¶ˆëŸ¬ì˜¤ê¸° (ê°™ì€ í´ë”)
try:
    from .config import MODEL_PATH, DEVICE, MODEL_CONFIG
except ImportError:
    # ê²½ë¡œ ë¬¸ì œ ì‹œ ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
    from ai.ml_api.api.services.config import MODEL_PATH, DEVICE, MODEL_CONFIG

_model = None


def load_model() -> torch.nn.Module:
    global _model

    if _model is not None:
        return _model

    if ComfortMLP is None:
        print("âŒ ComfortMLP í´ëž˜ìŠ¤ê°€ ì—†ì–´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ëª¨ë¸ ì´ˆê¸°í™”
    # MODEL_CONFIGê°€ ë”•ì…”ë„ˆë¦¬ë¼ë©´ **ë¥¼ ë¶™ì—¬ì„œ ì–¸íŒ¨í‚¹
    model = ComfortMLP(**MODEL_CONFIG)

    try:
        print(f"ðŸ”„ ëª¨ë¸ íŒŒì¼ ë¡œë”© ì¤‘: {MODEL_PATH}")
        state_dict = torch.load(
            MODEL_PATH,
            map_location=DEVICE,
            # weights_only=True # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
        )
        model.load_state_dict(state_dict)
        model.to(DEVICE)
        model.eval()

        _model = model
        print("âœ… íŒ€ì› ëª¨ë¸(ComfortMLP) ë¡œë“œ ì™„ë£Œ!")
        return _model
    except Exception as e:
        print(f"ðŸ”¥ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def get_model() -> torch.nn.Module:
    if _model is None:
        return load_model()
    return _model