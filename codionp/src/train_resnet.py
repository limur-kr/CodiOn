# train_resnet.py

'''Train Resnet on Cifar10 with PyTorch.'''
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.backends.cudnn as cudnn

import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
import torchvision.datasets as datasets
import torchvision.models as models

import os
import argparse
from PIL import Image, ImageFile
import time

# ðŸš© ìˆ˜ì •: ResNet ëª¨ë¸ì€ íŒ¨í‚¤ì§€ êµ¬ì¡°ì— ë”°ë¼ import í•©ë‹ˆë‹¤.
from resnet_model.resnet import get_resnet18
from utils import progress_bar
from os.path import abspath

ImageFile.LOAD_TRUNCATED_IMAGES = True

# -------------------- [ìˆ˜ì •ëœ ì½”ë“œ ë¸”ë¡: CustomDataset] --------------------
# Windows MAX_PATH ì œí•œì„ ìš°íšŒí•˜ê¸° ìœ„í•œ CustomImageFolder
WINDOWS_MAX_PATH = 259  # Windowsì˜ ê¸°ë³¸ ê²½ë¡œ ê¸¸ì´ ì œí•œ

class CustomImageFolder(ImageFolder):
    def __init__(self, root, transform=None, target_transform=None, loader=None, is_valid_file=None):
        # ImageFolderì˜ __init__ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
        super().__init__(root, transform=transform, target_transform=target_transform, loader=loader, is_valid_file=is_valid_file)

    # ðŸš© ìˆ˜ì •: make_dataset ì˜¤ë²„ë¼ì´ë“œ (allow_empty ì¸ìž ì¶”ê°€ ë° is_valid_file ì²˜ë¦¬ ê°œì„ )
    def make_dataset(self, directory, class_to_idx, extensions=None, is_valid_file=None, allow_empty=False):
        instances = []
        directory = os.path.expanduser(directory)

        # ImageFolderì˜ find_classes í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í´ëž˜ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        if class_to_idx is None:
            _, class_to_idx = self.find_classes(directory)

        # ðŸš© ìˆ˜ì •: ìœ íš¨ì„± ê²€ì‚¬ í•¨ìˆ˜ ì„¤ì •
        # self.is_valid_fileì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, ì¸ìžë¡œ ë°›ì€ ê°’ì„ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.
        if is_valid_file is None:
            if extensions is not None:
                def is_valid_file(x):
                    return x.lower().endswith(extensions)
            else:
                # ë§Œì•½ ë‘˜ ë‹¤ ì—†ë‹¤ë©´ ê¸°ë³¸ ì´ë¯¸ì§€ í™•ìž¥ìžë¥¼ ì‚¬ìš© (ì•ˆì „ ìž¥ì¹˜)
                from torchvision.datasets.folder import IMG_EXTENSIONS
                def is_valid_file(x):
                    return x.lower().endswith(IMG_EXTENSIONS)

        # ëª¨ë“  í´ëž˜ìŠ¤ í´ë”ë¥¼ íƒìƒ‰í•˜ë©° íŒŒì¼ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        for target_class in sorted(class_to_idx.keys()):
            class_index = class_to_idx[target_class]
            target_dir = os.path.join(directory, target_class)

            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):
                for fname in sorted(fnames):
                    path = os.path.join(root, fname)

                    # ðŸš© í•µì‹¬ ë¡œì§: ê²½ë¡œ ê¸¸ì´ê°€ ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ê±´ë„ˆëœë‹ˆë‹¤.
                    if len(path) >= WINDOWS_MAX_PATH:
                        print(f"[Excluded] Path length exceeds {WINDOWS_MAX_PATH} limit: {path}")
                        continue  # ì´ íŒŒì¼ì€ samples ëª©ë¡ì— ì¶”ê°€í•˜ì§€ ì•Šê³  ê±´ë„ˆëœë‹ˆë‹¤.

                    # í™•ìž¥ìž í•„í„°ë§ (ìˆ˜ì •ëœ is_valid_file ì‚¬ìš©)
                    if is_valid_file(path):
                        instances.append((path, class_index))

        return instances
# ---------------------------------------------------------------------------------


# define input args
parser = argparse.ArgumentParser(description='PyTorch ResNet Training')
parser.add_argument('--lr', default=0.01, type=float, help='learning rate')
parser.add_argument('--resume', '-r', action='store_true',
                    help='resume from checkpoint')
parser.add_argument('--data', default='cifar10', type=str,
                    help='dataset selection')
parser.add_argument('--batch_size', default=128, type=int,
                    help='batch size')
parser.add_argument('--num_classes', default=10, type=int,
                    help='number of classes')
parser.add_argument('--num_workers', default=2, type=int,
                    help='number of workers')
# use current path as data_parent_dir
parser.add_argument('--data_parent_dir', default='../data', type=str,
                    help='parent directory of data')

args = parser.parse_args()

DATA = args.data
# ðŸš© ìˆ˜ì •: ê²½ë¡œ ì„¤ì • ë¡œì§ì„ ì ˆëŒ€ ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ Windows í˜¸í™˜ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
data_path = os.path.join(base_dir, 'data', DATA)

train_data = os.path.join(data_path, 'train')
test_data = os.path.join(data_path, 'test')

device = 'cuda' if torch.cuda.is_available() else 'cpu'
best_acc = 0  # best test accuracy
start_epoch = 0  # start from epoch 0 or last checkpoint epoch

# Data
print('==> Preparing data..')

# data augmentation on train
data_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# test set transform
test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

print(f"Train data path: {train_data}")
print(f"Test data path: {test_data}")

# pytorch ImageFolder, dataset stores sample and its label;
# slip the ImageFolder data into dataloader
# ðŸš© ìˆ˜ì •: CustomImageFolder ì‚¬ìš© (loader ì¸ìžëŠ” ì œê±°í•˜ê³  ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë¨)
train_set = CustomImageFolder(root=train_data, transform=data_transform, loader=datasets.folder.pil_loader)
test_set = CustomImageFolder(root=test_data, transform=test_transform, loader=datasets.folder.pil_loader)

train_loader = torch.utils.data.DataLoader(
    train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)

test_loader = torch.utils.data.DataLoader(
    test_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)

# Model
print('==> Building model..')
# use ResNet18
# ðŸš© ìˆ˜ì •: num_classes ì¸ìˆ˜ë¥¼ ì‚¬ìš©
net = get_resnet18(args.num_classes)
net = net.to(device)

if device == 'cuda':
    # net = torch.nn.DataParallel(net)
    cudnn.benchmark = True

if args.resume:
    # Load checkpoint.
    print('==> Resuming from checkpoint..')
    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'
    checkpoint = torch.load('./checkpoint/ckpt.pth')
    net.load_state_dict(checkpoint['net'])
    best_acc = checkpoint['acc']
    start_epoch = checkpoint['epoch']

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=args.lr,
                      momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)


def topk(output, target, ks=(1,)):
    """Computes the precision@k for the specified values of k"""
    max_k = max(ks)
    batch_size = target.size(0)

    _, pred = output.topk(max_k, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in ks:
        correct_k = correct[:k].reshape(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res


# Training
def train(epoch):
    print('\nEpoch: %d' % epoch)
    model = net
    model.train()
    train_loss = 0
    correct = 0
    total = 0

    # ðŸš© ìˆ˜ì •: ê¸°ì¡´ for ë£¨í”„ ì‚¬ìš©
    for batch_idx, (inputs, targets) in enumerate(train_loader):

        # ðŸš© ìµœì¢… ìˆ˜ì •: ë°ì´í„° ë¡œë“œ ì‹œì ì˜ ì˜¤ë¥˜ê°€ ì•„ë‹Œ, inputs, targetsì„ deviceë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ”
        # Device-side ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ try-except ë¸”ë¡ ìœ ì§€
        try:
            # in pytorch, .to is transfer to (), here is move to device
            inputs, targets = inputs.to(device), targets.to(device)
        except Exception as e:
            # CustomImageFolderì—ì„œ ì´ë¯¸ ê²½ë¡œ ì˜¤ë¥˜ë¥¼ ê±¸ë €ê¸° ë•Œë¬¸ì—, ì´ ì˜¤ë¥˜ëŠ” ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ë¡œë”© ì˜¤ë¥˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
            print(f"\n[Skipped] Batch {batch_idx + 1} due to device or data error: {e}")
            continue  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë°°ì¹˜ë¡œ ë„˜ì–´ê°

        # except for the 1st loop, need to zero the gradient due to there is an auto differentian in . backward
        optimizer.zero_grad()
        # compute output, which is the label
        outputs = model(inputs)
        # use crossEntropyLoss to calculate loss
        loss = criterion(outputs, targets)
        # auto grad calculation
        loss.backward()
        # w = w + wg*lr
        optimizer.step()

        train_loss += loss.item()
        # pick the index of the max output
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
        tops = topk(outputs, targets, (1, 3, 5))

        progress_bar(batch_idx, len(train_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
                     % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))


def test(epoch):
    global best_acc
    model = net
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(test_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            tops = topk(outputs, targets, (1, 3, 5))

            progress_bar(batch_idx, len(test_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'
                         % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))

    # Save checkpoint.
    acc = 100. * correct / total
    if acc > best_acc:
        print('Saving..')
        state = {
            'net': model.state_dict(),
            'acc': acc,
            'epoch': epoch,
        }
        if not os.path.isdir('checkpoint'):
            os.mkdir('checkpoint')
        torch.save(state, './checkpoint/ckpt.pth')
        best_acc = acc


for epoch in range(start_epoch, start_epoch + 200):
    train(epoch)
    test(epoch)
    scheduler.step()